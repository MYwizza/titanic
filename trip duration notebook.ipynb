{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon May 14 09:25:19 2018\n",
    "\n",
    "@author: Wizza\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd  #pandas for using dataframe and reading csv \n",
    "import numpy as np   #numpy for vector operations and basic maths \n",
    "#import simplejson    #getting JSON in simplified format\n",
    "import urllib        #for url stuff\n",
    "#import gmaps       #for using google maps to visulalize places on maps\n",
    "import re            #for processing regular expressions\n",
    "import datetime      #for datetime operations\n",
    "import calendar      #for calendar for datetime operations\n",
    "import time          #to get the system time\n",
    "import scipy         #for other dependancies\n",
    "from sklearn.cluster import KMeans # for doing K-means clustering\n",
    "from haversine import haversine # for calculating haversine distance\n",
    "import math          #for basic maths operations\n",
    "import seaborn as sns #for making plots\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import os  # for os commands\n",
    "from scipy.misc import imread, imresize, imsave  # for plots \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from IPython.display import HTML\n",
    "from matplotlib.pyplot import *\n",
    "from matplotlib import cm\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import io\n",
    "import base64\n",
    "output_notebook()\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "\n",
    "address='https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial/code'\n",
    "\n",
    "#导入和合并数据\n",
    "#将训练数据，将数据train和fastest_routes_train_part_1/2中的数据进行合并\n",
    "#-----------------------------------------\n",
    "s=time.time()\n",
    "train_fr_1=pd.read_csv(r'C:\\Users\\Wizza\\Documents\\Python Scripts\\trip duration\\fastest_routes_train_part_1.csv')\n",
    "train_fr_2=pd.read_csv(r'C:\\Users\\Wizza\\Documents\\Python Scripts\\trip duration\\fastest_routes_train_part_2.csv')\n",
    "train_fr=pd.concat([train_fr_1,train_fr_2])\n",
    "train_fr_new=train_fr[['id','total_distance','total_travel_time','number_of_steps']]\n",
    "train_df=pd.read_csv(r'C:\\Users\\Wizza\\Documents\\Python Scripts\\trip duration\\train.csv')\n",
    "train=pd.merge(train_df,train_fr_new,on='id',how='left')\n",
    "train_df=train.copy()\n",
    "end=time.time()\n",
    "print('time taken by above cell is {}'.format((end-s)))\n",
    "train_df.head()\n",
    "#----------------------------------------\n",
    "\n",
    "#检查ID是否唯一\n",
    "start=time.time()\n",
    "train_data=train_df.copy()\n",
    "print('number of columns and rows are {} and {} respectively.'.format(train_data.shape[1],train_data.shape[0]))\n",
    "if np.unique(train_data['id']).shape[0]==train_data['id'].shape[0]:\n",
    "    print('ids are unique')\n",
    "print('number of nulls - {}'.format(train_data.isnull().sum().sum()))\n",
    "end=time.time()\n",
    "print('time taken by above cell is {}.'.format(end-start))\n",
    "\n",
    "\n",
    "#对将要预测的trip_duration进行可视化，由于数值较大，采取取对数显示，可视化的形式是直方图，密度分布\n",
    "#-----------------------------------------\n",
    "#预测变量行程时间可视化\n",
    "start=time.time()\n",
    "sns.set(style='white',palette='muted',color_codes=True)\n",
    "f,axes=plt.subplots(1,1,figsize=(11,7),sharex=True)\n",
    "sns.despine(left=True)\n",
    "sns.distplot(np.log(train_df['trip_duration'].values+1),\n",
    "             axlabel='log(trip_duration)',label='log(trip_duration)',bins=50,color='r')\n",
    "plt.setp(axes,yticks=[])\n",
    "plt.tight_layout()\n",
    "end=time.time()\n",
    "print('time taken by above cell is {}.'.format((end-start)))\n",
    "plt.show()\n",
    "#结论：trip_duration普遍分布在e^4-1~e^8-1之间\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#对参数特征经纬度进行可视化显示，可视化形式是直方图密度分布\n",
    "#-----------------------------------------\n",
    "#经纬度分析\n",
    "start=time.time()\n",
    "sns.set(style='white',palette='deep',color_codes=True)\n",
    "f,axes=plt.subplots(2,2,figsize=(10,10))\n",
    "sns.despine(left=True)\n",
    "sns.distplot(train_df['pickup_latitude'].values,label='pickup_latitude',color='m',bins=100,ax=axes[0,0])\n",
    "sns.distplot(train_df['pickup_longitude'].values,label='pickup_longitide',color='m',bins=100,ax=axes[0,1])\n",
    "sns.distplot(train_df['dropoff_latitude'].values,label='dropoff_latitude',color='m',bins=100,ax=axes[1,0])\n",
    "sns.distplot(train_df['dropoff_longitude'].values,label='dropoff_longitide',color='m',bins=100,ax=axes[1,1])\n",
    "plt.setp(axes,yticks=[])\n",
    "plt.tight_layout()\n",
    "end=time.time()\n",
    "print('time taken by above cell is {}.'.format((end-start)))\n",
    "plt.show()\n",
    "#可视化结论：上车和下车纬度（latitude）集中于40~42之间，经度（longitude）集中于-76~-73之间\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#根据上下车经纬度可视化结果，选取集中度较大的数据作为有效数据，剔除边缘值\n",
    "#-----------------------------------------\n",
    "#缩小经纬度范围\n",
    "df=train_df.loc[(train_df.pickup_latitude>40.6)&(train_df.pickup_latitude<40.9)]\n",
    "df=df.loc[(df.dropoff_latitude>40.6)&(df.dropoff_latitude<40.9)]\n",
    "df=df.loc[(df.dropoff_longitude>-74.05)&(df.dropoff_longitude<-73.7)]\n",
    "df=df.loc[(df.pickup_longitude>-74.05)&(df.pickup_longitude<-73.7)]\n",
    "train_data_new=df.copy()\n",
    "sns.set(style='white',palette='muted',color_codes=True)\n",
    "f,axes=plt.subplots(2,2,figsize=(12,12))\n",
    "sns.despine(left=True)\n",
    "sns.distplot(train_data_new['pickup_latitude'].values,label='pickup_latitude',color='m',bins=100,ax=axes[0,0])\n",
    "sns.distplot(train_data_new['pickup_longitude'].values,label='pickup_longitide',color='g',bins=100,ax=axes[0,1])\n",
    "sns.distplot(train_data_new['dropoff_latitude'].values,label='dropoff_latitude',color='m',bins=100,ax=axes[1,0])\n",
    "sns.distplot(train_data_new['dropoff_longitude'].values,label='dropoff_longitide',color='g',bins=100,ax=axes[1,1])\n",
    "plt.setp(axes,yticks=[])\n",
    "plt.tight_layout()\n",
    "print(df.shape[0],train_data.shape[0])\n",
    "#结论：latitude：40.6~40.9,longitude:-74.05~-73.7\n",
    "#-----------------------------------------\n",
    "\n",
    "#增加上车日期的特征，将年月日时分秒变为年月日\n",
    "#-----------------------------------------\n",
    "temp=train_data.copy()\n",
    "train_data['pickup_datetime']=pd.to_datetime(train_data.pickup_datetime)\n",
    "train_data.loc[:,'pick_date']=train_data['pickup_datetime'].dt.date\n",
    "train_data.head()\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#可视化不同vendor_id下，每天trip_duration的平均值，可视化方法，线型\n",
    "#-----------------------------------------\n",
    "ts_v1=pd.DataFrame(train_data.loc[train_data['vendor_id']==1].groupby('pick_date')['trip_duration'].mean())\n",
    "ts_v1.reset_index(inplace=True)\n",
    "ts_v2=pd.DataFrame(train_data.loc[train_data['vendor_id']==2].groupby('pick_date')['trip_duration'].mean())\n",
    "ts_v2.reset_index(inplace=True)\n",
    "\n",
    "from bokeh.palettes import Spectral4\n",
    "from bokeh.plotting import figure,output_notebook,show\n",
    "\n",
    "output_notebook()\n",
    "p=figure(plot_width=800, plot_height=250, x_axis_type=\"datetime\")\n",
    "p.title.text = 'Click on legend entries to hide the corresponding lines'\n",
    "\n",
    "for data, name, color in zip([ts_v1, ts_v2], [\"vendor 1\", \"vendor 2\"], Spectral4):\n",
    "    df = data\n",
    "    p.line(df['pick_date'], df['trip_duration'], line_width=2, color=color, alpha=0.8, legend=name)\n",
    "\n",
    "p.legend.location = \"top_left\"\n",
    "p.legend.click_policy=\"hide\"\n",
    "show(p)\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#通过地图可视化上车地点，同一上车地点的个数大于50，显示红色，大于10，显示黄色，小于10，显示绿色\n",
    "#-----------------------------------------\n",
    "#显示地图坐标\n",
    "rgb=np.zeros((3000,3500,3),dtype=np.uint8)\n",
    "train_data_new['pick_lat_new']=list(map(int,(train_data_new['pickup_latitude']-40.6)*10000))\n",
    "train_data_new['drop_lat_new'] = list(map(int, (train_data_new['dropoff_latitude'] - (40.6000))*10000))\n",
    "train_data_new['pick_lon_new'] = list(map(int, (train_data_new['pickup_longitude'] - (-74.050))*10000))\n",
    "train_data_new['drop_lon_new'] = list(map(int,(train_data_new['dropoff_longitude'] - (-74.050))*10000))\n",
    "\n",
    "summary_plot=pd.DataFrame(train_data_new.groupby(['pick_lat_new','pick_lon_new'])['id'].count())\n",
    "summary_plot.reset_index(inplace=True)\n",
    "summary_plot.head(120)\n",
    "lat_list=summary_plot['pick_lat_new'].unique()\n",
    "\n",
    "for i in lat_list:\n",
    "    lon_list=summary_plot.loc[summary_plot['pick_lat_new']==i]['pick_lon_new'].tolist()#获得每一个纬度下的经度列表\n",
    "    unit=summary_plot.loc[summary_plot['pick_lat_new']==i]['id'].tolist()#获得每一个纬度下，不同经度的ID个数列表\n",
    "    for j in lon_list:\n",
    "        a=unit[lon_list.index(j)]\n",
    "        if(a//50)>0:\n",
    "            rgb[i][j][0]=255\n",
    "            rgb[i,j,1]=0\n",
    "            rgb[i,j,2]=255\n",
    "        elif(a//10)>0:\n",
    "            rgb[i,j,0]=0\n",
    "            rgb[i,j,1]=255\n",
    "            rgb[i,j,2]=0\n",
    "        else:\n",
    "            rgb[i,j,0]=255\n",
    "            rgb[i,j,1]=0\n",
    "            rgb[i,j,2]=0\n",
    "fig,ax=plt.subplots(nrows=1,ncols=1,figsize=(14,20))\n",
    "ax.imshow(rgb,cmap='hot')\n",
    "ax.set_axis_off()\n",
    "#-----------------------------------------\n",
    "\n",
    "#-----------------------------------------\n",
    "rgb1=np.zeros((3000,3500,3),dtype=np.uint8)\n",
    "for k in range(len(summary_plot['id'])):\n",
    "    a=summary_plot['id']\n",
    "    i=summary_plot['pick_lat_new'][k]\n",
    "    j=summary_plot['pick_lon_new'][k]\n",
    "    a1=summary_plot['id'][k]\n",
    "    if(a1//50)>0:\n",
    "        rgb1[i][j][0]=255\n",
    "        rgb1[i,j,1]=0\n",
    "        rgb1[i,j,2]=255\n",
    "    elif(a1//10)>0:\n",
    "        rgb1[i,j,0]=0\n",
    "        rgb1[i,j,1]=255\n",
    "        rgb1[i,j,2]=0\n",
    "    else:\n",
    "        rgb1[i,j,0]=255\n",
    "        rgb1[i,j,1]=0\n",
    "        rgb1[i,j,2]=0\n",
    "\n",
    "fig,ax=plt.subplots(nrows=1,ncols=1,figsize=(14,20))\n",
    "ax.imshow(rgb1,cmap='hot')\n",
    "ax.set_axis_off()\n",
    "#finds：红色点表明<10个的trips作为起点，绿色表明10~50个的trips作为起点，黄色表明>50的trips作为起点\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#提取距离特征，上车到下车地点的hvsine距离和manhtn距离，提取方向的特征。\n",
    "#-----------------------------------------\n",
    "#特征提取\n",
    "def haversine_(lat1,lng1,lat2,lng2):\n",
    "    #该函数用来计算两个经纬度之间的距离\n",
    "    lat1,lng1,lat2,lng2=map(np.radians,(lat1,lng1,lat2,lng2))\n",
    "    AVG_EARTH_RADIUS=6371 #KM\n",
    "    lat=lat2-lat1\n",
    "    lng=lng2-lng1\n",
    "    d=np.sin(lat*0.5)**2+np.cos(lat1)*np.cos(lat2)*np.sin(lng*0.5)**2\n",
    "    h=2*AVG_EARTH_RADIUS*np.arcsin(np.sqrt(d))\n",
    "    return(h)\n",
    "\n",
    "def manhattan_distance_pd(lat1,lng1,lat2,lng2):\n",
    "    #该函数用来计算曼哈顿距离\n",
    "    a=haversine_(lat1,lng1,lat1,lng2)\n",
    "    b=haversine_(lat1,lng1,lat2,lng1)\n",
    "    return a+b\n",
    "\n",
    "def bearing_array(lat1,lng1,lat2,lng2):\n",
    "    #该函数用于计算方位\n",
    "    lng_delta_rad=np.radians(lng2-lng1)\n",
    "    lat1,lng1,lat2,lng2=map(np.radians,(lat1,lng1,lat2,lng2))\n",
    "    y=np.sin(lng_delta_rad)*np.cos(lat2)\n",
    "    x=np.cos(lat1)*np.sin(lat2)-np.sin(lat1)*np.cos(lat2)*np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y,x))\n",
    "\n",
    "#提取时间的特征，上车的月份，上车的Hour时刻，上车是一年的第几周，上车是一年的第几天，上车是一星期的第几天\n",
    "#-----------------------------------------\n",
    "train_data=temp.copy()\n",
    "train_data['pickup_datetime']=pd.to_datetime(train_data.pickup_datetime)\n",
    "train_data.loc[:,'pick_month']=train_data['pickup_datetime'].dt.month\n",
    "train_data.loc[:,'pick_hour']=train_data['pickup_datetime'].dt.hour\n",
    "train_data.loc[:,'week_of_year']=train_data['pickup_datetime'].dt.weekofyear\n",
    "train_data.loc[:,'day_of_year']=train_data['pickup_datetime'].dt.dayofyear\n",
    "train_data.loc[:,'day_of_week']=train_data['pickup_datetime'].dt.dayofweek\n",
    "train_data.loc[:,'hvsine_pick_drop']=haversine_(train_data['pickup_latitude'].values\n",
    "              ,train_data['pickup_longitude'].values,train_data['dropoff_latitude'].values,\n",
    "              train_data['dropoff_longitude'].values)\n",
    "train_data.loc[:,'manhtn_pick_drop']=manhattan_distance_pd(train_data['pickup_latitude'].values\n",
    "              ,train_data['pickup_longitude'].values,train_data['dropoff_latitude'].values,\n",
    "              train_data['dropoff_longitude'].values)\n",
    "train_data.loc[:,'bearing']=bearing_array(train_data['pickup_latitude'].values\n",
    "              ,train_data['pickup_longitude'].values,train_data['dropoff_latitude'].values,\n",
    "              train_data['dropoff_longitude'].values)\n",
    "#-----------------------------------------\n",
    "\n",
    "#可视化每个经纬度的上车地点的个数随小时数的变化图\n",
    "#-----------------------------------------\n",
    "def color(hour):\n",
    "    return hour*10\n",
    "\n",
    "def Animation(hour,temp):\n",
    "    train_data_new=temp.loc[temp['hour']==hour]\n",
    "    rgb1=np.zeros((3000,3500,3),dtype=np.uint8)\n",
    "    train_data_new['pick_lat_new']=list(map(int,(train_data_new['pickup_latitude']-40.6)*10000))\n",
    "    train_data_new['drop_lat_new']=list(map(int,(train_data_new['dropoff_latitude']-40.6)*10000))\n",
    "    train_data_new['pick_lon_new']=list(map(int,(train_data_new['pickup_longitude']-(-74.05))*10000))\n",
    "    train_data_new['drop_lon_new']=list(map(int,(train_data_new['dropoff_longitude']-(-74.05))*10000))\n",
    "    summary_plot=pd.DataFrame(train_data_new.groupby(['pick_lat_new','pick_lon_new'])['id'].count())\n",
    "    \n",
    "    summary_plot.reset_index(inplace=True)\n",
    "    summary_plot.head(120)\n",
    "    \n",
    "    for k in range(len(summary_plot['id'])):\n",
    "        i=summary_plot['pick_lat_new'][k]\n",
    "        j=summary_plot['pick_lon_new'][k]\n",
    "        a1=summary_plot['id'][k]\n",
    "        if(a1//50)>0:\n",
    "            rgb1[i][j][0]=255-color(hour)\n",
    "            rgb1[i,j,1]=255-color(hour)\n",
    "            rgb1[i,j,2]=0+color(hour)\n",
    "        elif(a1//10)>0:\n",
    "            rgb1[i,j,0]=0+color(hour)\n",
    "            rgb1[i,j,1]=255-color(hour)\n",
    "            rgb1[i,j,2]=0+color(hour)\n",
    "        else:\n",
    "            rgb1[i,j,0]=255-color(hour)\n",
    "            rgb1[i,j,1]=0+color(hour)\n",
    "            rgb1[i,j,2]=0+color(hour)\n",
    "    \n",
    "    return rgb1\n",
    "\n",
    "images_list=[]\n",
    "train_data_new['pickup_datetime']=pd.to_datetime(train_data_new['pickup_datetime'])\n",
    "train_data_new.loc[:,'hour']=train_data_new['pickup_datetime'].dt.hour\n",
    "\n",
    "for i in list(range(0,24)):\n",
    "    im=Animation(i,train_data_new)\n",
    "    images_list.append(im)\n",
    "\n",
    "#画出上车地点随小时的变化动图\n",
    "def build_gif(imgs=images_list,show_gif=False,save_gif=True,title=''):\n",
    "    fig,ax=plt.subplots(nrows=1,ncols=1,figsize=(10,10))\n",
    "    ax.set_axis_off()\n",
    "    hr_range=(list(range(0,24)))\n",
    "    \n",
    "    def show_im(pairs):\n",
    "        ax.clear()\n",
    "        ax.set_title('absolute traffic-hour'+str(int(pairs[0]))+':00')\n",
    "        ax.imshow(pairs[1])\n",
    "        ax.set_axis_off()\n",
    "    \n",
    "    pairs=list(zip(hr_range,imgs))\n",
    "    im_ani=animation.FuncAnimation(fig,show_im,pairs,interval=500,repeat_delay=0,blit=False)\n",
    "    plt.cla()\n",
    "    if save_gif:\n",
    "        im_ani.save('animation.html',writer='imagemagick')\n",
    "    if show_gif:\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "build_gif()\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "filename = 'animation.html'\n",
    "video = io.open(filename, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))\n",
    "#findings：上午2点到6点出行较少，上午7点到下午4点出行为中等，下午5点到凌晨1点出行较多\n",
    "\n",
    "#可视化，一周的每一天与trip时间平均值的关系的关系\n",
    "summary_wdays_avg_duration=pd.DataFrame(train_data.groupby(['vendor_id','day_of_week'])['trip_duration'].mean())\n",
    "summary_wdays_avg_duration.reset_index(inplace=True)\n",
    "summary_wdays_avg_duration['unit']=1\n",
    "sns.set(style='white',palette='muted',color_codes=True)\n",
    "sns.set_context('poster')\n",
    "sns.tsplot(data=summary_wdays_avg_duration,time='day_of_week',unit='unit',condition='vendor_id',\n",
    "           value='trip_duration')\n",
    "sns.despine(bottom=False)\n",
    "#findings\n",
    "#一周的每一天，vendor 1比vendor2的trip时间长，介于1~250之间\n",
    "\n",
    "#可视化，利用琴型图（violinplot）分析乘客数与trip时间的关系\n",
    "sns.set(style='white',palette='pastel',color_codes=True)\n",
    "sns.set_context('poster')\n",
    "train_data2=train_data.copy()\n",
    "train_data2['trip_duration']=np.log(train_data['trip_duration'])\n",
    "sns.violinplot(x='passenger_count',y='trip_duration',hue='vendor_id',data=train_data2,inner='quart',split=True,\n",
    "               palette={1:'g',2:'r'})\n",
    "sns.despine(left=True)\n",
    "#findings\n",
    "#当乘客数是0时，vendor1和2都会有trip时间为负值，应去除\n",
    "#乘客数为0的解释是，乘客提前叫了一辆出租车，在等待\n",
    "#vendor1在人数是2和3的，有长的trip时间\n",
    "#人数为7，8，9的trip duration非常少\n",
    "\n",
    "#可视化，箱型图猜测一周每一天与trip总时间的关系\n",
    "sns.set(style='ticks')\n",
    "sns.boxplot(x='day_of_week',y='trip_duration',hue='vendor_id',data=train_data,palette='PRGn')\n",
    "plt.ylim(0,6000)\n",
    "sns.despine(trim=True,offset=10)\n",
    "#findings\n",
    "#周1，2，3，4比其他天的trip duration 长\n",
    "\n",
    "#可视化，line-plots分析一周每一天按小时的平均trip duration变化\n",
    "summary_hour_duration=pd.DataFrame(train_data.groupby(['day_of_week','pick_hour'])['trip_duration'].mean())\n",
    "summary_hour_duration.reset_index(inplace=True)\n",
    "summary_hour_duration['unit']=1\n",
    "sns.set(style='white',palette='muted',color_codes=False)\n",
    "sns.tsplot(time='pick_hour',unit='unit',condition='day_of_week',value='trip_duration',data=summary_hour_duration)\n",
    "sns.despine(bottom=False)\n",
    "#findings\n",
    "#在am5：00~15：00时，周六和周日的trip duration明显比工作日少\n",
    "#周六的深夜 trip dutation比其他的时间多\n",
    "#-----------------------------------------\n",
    "\n",
    "#集群分析，通过指定初始值，划分20个集群，通过KMeans算法进行集群模型建模，获得两个特征，上车集群label和下车集群label\n",
    "#-----------------------------------------\n",
    "#集群分析（cluster）\n",
    "def assign_cluster(df,k):\n",
    "    df_pick=df[['pickup_longitude','pickup_latitude']]\n",
    "    df_drop=df[['dropoff_longitude','dropoff_latitude']]\n",
    "    init=np.array([[ -73.98737616,   40.72981533],\n",
    "       [-121.93328857,   37.38933945],\n",
    "       [ -73.78423222,   40.64711269],\n",
    "       [ -73.9546417 ,   40.77377538],\n",
    "       [ -66.84140269,   36.64537175],\n",
    "       [ -73.87040541,   40.77016484],\n",
    "       [ -73.97316185,   40.75814346],\n",
    "       [ -73.98861094,   40.7527791 ],\n",
    "       [ -72.80966949,   51.88108444],\n",
    "       [ -76.99779701,   38.47370625],\n",
    "       [ -73.96975298,   40.69089596],\n",
    "       [ -74.00816622,   40.71414939],\n",
    "       [ -66.97216034,   44.37194443],\n",
    "       [ -61.33552933,   37.85105133],\n",
    "       [ -73.98001393,   40.7783577 ],\n",
    "       [ -72.00626526,   43.20296402],\n",
    "       [ -73.07618713,   35.03469086],\n",
    "       [ -73.95759366,   40.80316361],\n",
    "       [ -79.20167796,   41.04752096],\n",
    "       [ -74.00106031,   40.73867723]])\n",
    "    k_means_pick=KMeans(n_clusters=k,init=init,n_init=1)\n",
    "    k_means_pick.fit(df_pick)\n",
    "    clust_pick=k_means_pick.labels_#获得训练数据的集群label\n",
    "    df['label_pick']=clust_pick.tolist()\n",
    "    df['label_drop']=k_means_pick.predict(df_drop)#获得测试数据的集群label\n",
    "    return df,k_means_pick\n",
    "\n",
    "train_cl,k_means=assign_cluster(train_data,20)\n",
    "#-----------------------------------------\n",
    "\n",
    "#增加4个特征，上车集群点的经纬度，和下车集群点的经纬度\n",
    "#-----------------------------------------\n",
    "centroid_pickups=pd.DataFrame(k_means.cluster_centers_,columns=['centroid_pick_long','centroid_pick_lat'])\n",
    "centroid_dropoff=pd.DataFrame(k_means.cluster_centers_,columns=['centroid_drop_long','centroid_drop_lat'])\n",
    "centroid_pickups['label_pick']=centroid_pickups.index\n",
    "centroid_dropoff['label_drop']=centroid_dropoff.index\n",
    "train_cl=pd.merge(train_cl,centroid_pickups,how='left',on='label_pick')\n",
    "train_cl=pd.merge(train_cl,centroid_dropoff,how='left',on='label_drop')\n",
    "#-----------------------------------------\n",
    "\n",
    "#增加9个特征，上车地点—上车中心，下车地点到下车中心，上车中心到下车中心的hvsine值\n",
    "#上车地点—上车中心，下车地点到下车中心，上车中心到下车中心的manhatn值\n",
    "#上车地点—上车中心，下车地点到下车中心，上车中心到下车中心的方位值\n",
    "#-----------------------------------------\n",
    "#根据集群，计算上下车的距离和方位\n",
    "train_cl.loc[:,'hvsine_pick_cent_p']=haversine_(train_cl['pickup_latitude'].values,train_cl['pickup_longitude'].values,\n",
    "            train_cl['centroid_pick_lat'].values,train_cl['centroid_pick_long'].values)#计算上车地点只集群点的距离\n",
    "train_cl.loc[:,'hvsine_drop_cent_d']=haversine_(train_cl['dropoff_latitude'].values,train_cl['dropoff_longitude'].values,\n",
    "            train_cl['centroid_drop_lat'].values,train_cl['centroid_drop_long'].values)#计算下车地点至集群点的距离\n",
    "train_cl.loc[:,'hvsine_cent_p_cent_d']=haversine_(train_cl['centroid_pick_lat'].values,train_cl['centroid_pick_long'].values,\n",
    "            train_cl['centroid_drop_lat'].values,train_cl['centroid_drop_long'].values)#计算上车集群点和下车集群点之间的距离\n",
    "\n",
    "train_cl.loc[:,'manhtn_pick_cent_p']=manhattan_distance_pd(train_cl['pickup_latitude'].values,train_cl['pickup_longitude'].values,\n",
    "            train_cl['centroid_pick_lat'].values,train_cl['centroid_pick_long'].values)#计算上车地点只集群点的距离\n",
    "train_cl.loc[:,'manhtn_drop_cent_d']=manhattan_distance_pd(train_cl['dropoff_latitude'].values,train_cl['dropoff_longitude'].values,\n",
    "            train_cl['centroid_drop_lat'].values,train_cl['centroid_drop_long'].values)#计算下车地点至集群点的距离\n",
    "train_cl.loc[:,'manhtn_cent_p_cent_d']=manhattan_distance_pd(train_cl['centroid_pick_lat'].values,train_cl['centroid_pick_long'].values,\n",
    "            train_cl['centroid_drop_lat'].values,train_cl['centroid_drop_long'].values)#计算上车集群点和下车集群点之间的距离\n",
    "\n",
    "train_cl.loc[:,'bearing_pick_cent_p']=bearing_array(train_cl['pickup_latitude'].values,train_cl['pickup_longitude'].values,\n",
    "            train_cl['centroid_pick_lat'].values,train_cl['centroid_pick_long'].values)#计算上车地点只集群点的方位\n",
    "train_cl.loc[:,'bearing_drop_cent_d']=bearing_array(train_cl['dropoff_latitude'].values,train_cl['dropoff_longitude'].values,\n",
    "            train_cl['centroid_drop_lat'].values,train_cl['centroid_drop_long'].values)#计算下车地点至集群点的方位\n",
    "train_cl.loc[:,'bearing_cent_p_cent_d']=bearing_array(train_cl['centroid_pick_lat'].values,train_cl['centroid_pick_long'].values,\n",
    "            train_cl['centroid_drop_lat'].values,train_cl['centroid_drop_long'].values)#计算上车集群点和下车集群点之间的方位\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#增加2个特征，上车地点到下车地点的havsine平均速度\n",
    "#上车地点到下车地点的manhtn平均速度\n",
    "#-----------------------------------------\n",
    "train_cl['speed_hvsn']=train_cl.hvsine_pick_drop/train_cl.total_travel_time\n",
    "train_cl['speed_manhtn']=train_cl.manhtn_pick_drop/train_cl.total_travel_time\n",
    "train_cl.head()\n",
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "def cluster_summary(sum_df):\n",
    "    #该函数求每一个集群点的trip平均时间和最多的起始点行程次数\n",
    "    summary_avg_time=pd.DataFrame(sum_df.groupby('label_pick')['trip_duration'].mean())\n",
    "    summary_avg_time.reset_index(inplace=True)\n",
    "    summary_pref_clus=pd.DataFrame(sum_df.groupby(['label_pick','label_drop'])['id'].count())\n",
    "    summary_pref_clus.reset_index(inplace=True)\n",
    "    summary_pref_clus=summary_pref_clus.loc[summary_pref_clus.groupby('label_pick')['id'].idxmax()]\n",
    "    summary=pd.merge(summary_avg_time,summary_pref_clus,how='left',on='label_pick')\n",
    "    summary=summary.rename(columns={'trip_duration':'avg_triptime'})\n",
    "    return summary\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "#可视化，画出上车集群点的行程数大于70000的集群点，并挑选50个画出集群的上车和下车经纬度点\n",
    "#-----------------------------------------\n",
    "import folium\n",
    "def show_fmaps(train_data,path=1):\n",
    "    full_data=train_data\n",
    "    summary_full_data=pd.DataFrame(full_data.groupby('label_pick')['id'].count())\n",
    "    summary_full_data.reset_index(inplace=True)\n",
    "    summary_full_data=summary_full_data.loc[summary_full_data['id']>70000]\n",
    "    map_1=folium.Map(location=[40.7679,-73.9821],zoom_start=10,tiles='Stamen Toner')\n",
    "    new_df=train_data.loc[train_data['label_pick'].isin(summary_full_data.label_pick.tolist())].sample(50)\n",
    "    new_df.reset_index(inplace=True,drop=True)\n",
    "    for i in range(new_df.shape[0]):\n",
    "#        pick_long=new_df.loc[i,'pickup_longitude']\n",
    "#        pick_lat=new_df.loc[i,'pickup_latitude']\n",
    "#        dest_long=new_df.loc[i,'dropoff_longitude']\n",
    "#        dest_lat=new_df.loc[i,'dropoff_latitude']\n",
    "        pick_long = new_df.loc[new_df.index ==i]['pickup_longitude'].values[0]\n",
    "        pick_lat = new_df.loc[new_df.index ==i]['pickup_latitude'].values[0]\n",
    "        dest_long = new_df.loc[new_df.index ==i]['dropoff_longitude'].values[0]\n",
    "        dest_lat = new_df.loc[new_df.index ==i]['dropoff_latitude'].values[0]\n",
    "        folium.Marker([pick_lat,pick_long]).add_to(map_1)\n",
    "        folium.Marker([dest_lat,dest_long]).add_to(map_1)\n",
    "    return map_1\n",
    "\n",
    "osm=show_fmaps(train_data,path=1)\n",
    "osm.save('map.html')\n",
    "#----------------------------------------- \n",
    "\n",
    "#-----------------------------------------\n",
    "def clusters_map(clus_data,full_data,tile='OpenStreetMap',sig=0,zoom=12,circle=0,radius_=30):\n",
    "    map_1=folium.Map(location=[40.7679,-73.9821],zoom_start=zoom,tiles=tile)\n",
    "    summary_full_data=pd.DataFrame(full_data.groupby('label_pick')['id'].count())\n",
    "    summary_full_data.reset_index(inplace=True)\n",
    "    if sig==1:\n",
    "        summary_full_data=summary_full_data.loc[summary_full_data['id']>70000]\n",
    "    sig_cluster=summary_full_data['label_pick'].tolist()\n",
    "    clus_summary=cluster_summary(full_data)\n",
    "    for i in sig_cluster:\n",
    "        pick_long=clus_data.loc[clus_data.index==i]['centroid_pick_long'].values[0]\n",
    "        pick_lat=clus_data.loc[clus_data.index==i]['centroid_pick_lat'].values[0]\n",
    "        clus_no=clus_data.loc[clus_data.index==i]['label_pick'].values[0]\n",
    "        most_visited_clus=clus_summary.loc[clus_summary['label_pick']==i]['label_drop'].values[0]\n",
    "        avg_triptime=clus_summary.loc[clus_summary['label_pick']==i]['avg_triptime'].values[0]\n",
    "        pop='cluster='+str(clus_no)+'&most visited cluster='+str(most_visited_clus)+'&avg triptime from this cluster='+str(avg_triptime)\n",
    "        if circle==1:\n",
    "            folium.CircleMarker(location=[pick_lat,pick_long],radius=radius_,\n",
    "                                color='#F08080',\n",
    "                                fill_color='#3186cc',popup=pop).add_to(map_1)\n",
    "        folium.Marker([pick_lat,pick_long],popup=pop).add_to(map_1)\n",
    "    return map_1\n",
    "\n",
    "clus_map=clusters_map(centroid_pickups,train_cl,sig=0,zoom=3.2,circle=1,tile='Stamen Terrain')\n",
    "clus_map.save('clus_map.html')\n",
    "\n",
    "clus_map_sig=clusters_map(centroid_pickups,train_cl,sig=1,circle=1)\n",
    "clus_map_sig.save('clus_map_sig.html')        \n",
    "\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "parallel_coordinates(train_data.sample(1200)[['vendor_id','day_of_week','passenger_count',\n",
    "                     'pick_month','label_pick','pick_hour']],'vendor_id',colormap='rainbow')\n",
    "plt.show()\n",
    "        \n",
    "#从测试数据中提取特征\n",
    "\n",
    "fastest_routes_test=pd.read_csv(r'C:\\Users\\Wizza\\Documents\\Python Scripts\\trip duration\\fastest_routes_test.csv')\n",
    "test=pd.read_csv(r'C:\\Users\\Wizza\\Documents\\Python Scripts\\trip duration\\test.csv')\n",
    "\n",
    "test_sc=fastest_routes_test[['id','total_distance','total_travel_time','number_of_steps']]\n",
    "test_new=pd.merge(test,test_sc,on='id',how='left')\n",
    "\n",
    "test_new['pickup_datetime']=pd.to_datetime(test_new['pickup_datetime'])\n",
    "test_new['pick_hour']=test_new['pickup_datetime'].dt.hour\n",
    "test_new['pick_month']=test_new['pickup_datetime'].dt.month\n",
    "test_new['day_of_week']=test_new['pickup_datetime'].dt.dayofweek\n",
    "test_new['day_of_year']=test_new['pickup_datetime'].dt.dayofyear\n",
    "test_new['week_of_year']=test_new['pickup_datetime'].dt.weekofyear\n",
    "\n",
    "test_new['hvsine_pick_drop']=haversine_(test_new['pickup_latitude'].values,test_new['pickup_longitude'].values,\n",
    "        test_new['dropoff_latitude'].values,test_new['dropoff_longitude'].values)\n",
    "test_new['manhtn_pick_drop']=manhattan_distance_pd(test_new['pickup_latitude'].values,test_new['pickup_longitude'].values,\n",
    "        test_new['dropoff_latitude'].values,test_new['dropoff_longitude'].values)\n",
    "test_new['bearing']=bearing_array(test_new['pickup_latitude'].values,test_new['pickup_longitude'].values,\n",
    "        test_new['dropoff_latitude'].values,test_new['dropoff_longitude'].values)\n",
    "\n",
    "test_new['label_pick']=k_means.predict(test_new[['pickup_latitude','pickup_longitude']])\n",
    "test_new['label_drop']=k_means.predict(test_new[['dropoff_latitude','dropoff_longitude']])\n",
    "test_cl=pd.merge(test_new,centroid_pickups,how='left',on='label_pick')\n",
    "test_cl=pd.merge(test_cl,centroid_dropoff,how='left',on='label_drop')\n",
    "\n",
    "test_cl['hvsine_pick_cent_p']=haversine_(test_cl['pickup_latitude'].values,test_cl['pickup_longitude'].values,\n",
    "       test_cl['centroid_pick_lat'].values,test_cl['centroid_pick_long'].values)\n",
    "test_cl['hvsine_drop_cent_d']=haversine_(test_cl['dropoff_latitude'].values,test_cl['dropoff_longitude'].values,\n",
    "       test_cl['centroid_drop_lat'].values,test_cl['centroid_drop_long'].values)\n",
    "test_cl['hvsine_cent_p_cent_d']=haversine_(test_cl['centroid_pick_lat'].values,test_cl['centroid_pick_long'].values,\n",
    "       test_cl['centroid_drop_lat'].values,test_cl['centroid_drop_long'].values)\n",
    "\n",
    "test_cl['manhtn_pick_cent_p']=manhattan_distance_pd(test_cl['pickup_latitude'].values,test_cl['pickup_longitude'].values,\n",
    "       test_cl['centroid_pick_lat'].values,test_cl['centroid_pick_long'].values)\n",
    "test_cl['manhtn_drop_cent_d']=manhattan_distance_pd(test_cl['dropoff_latitude'].values,test_cl['dropoff_longitude'].values,\n",
    "       test_cl['centroid_drop_lat'].values,test_cl['centroid_drop_long'].values)\n",
    "test_cl['manhtn_cent_p_cent_d']=manhattan_distance_pd(test_cl['centroid_pick_lat'].values,test_cl['centroid_pick_long'].values,\n",
    "       test_cl['centroid_drop_lat'].values,test_cl['centroid_drop_long'].values)\n",
    "\n",
    "test_cl['bearing_pick_cent_p']=bearing_array(test_cl['pickup_latitude'].values,test_cl['pickup_longitude'].values,\n",
    "       test_cl['centroid_pick_lat'].values,test_cl['centroid_pick_long'].values)\n",
    "test_cl['bearing_drop_cent_d']=bearing_array(test_cl['dropoff_latitude'].values,test_cl['dropoff_longitude'].values,\n",
    "       test_cl['centroid_drop_lat'].values,test_cl['centroid_drop_long'].values)\n",
    "test_cl['bearing_cent_p_cent_d']=bearing_array(test_cl['centroid_pick_lat'].values,test_cl['centroid_pick_long'].values,\n",
    "       test_cl['centroid_drop_lat'].values,test_cl['centroid_drop_long'].values)\n",
    "\n",
    "test_cl['speed_hvsn']=test_cl['hvsine_pick_drop']/test_cl['total_travel_time']\n",
    "test_cl['speed_manhtn']=test_cl['manhtn_pick_drop']/test_cl['total_travel_time']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#利用主成份分析的方法PCA(),获得上车地点的主成为（即将经度和纬度两个特征，提取每个特征的成分比）\n",
    "#通过PCA增加了上车地点的两个主成份，下车地点的两个主成份，\n",
    "#-----------------------------------------\n",
    "#建立XGB模型\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import warnings\n",
    "\n",
    "train=train_cl\n",
    "test=test_cl\n",
    "coords=np.vstack((train[['pickup_latitude','pickup_longitude']].values,\n",
    "                 train[['dropoff_latitude','dropoff_longitude']].values,\n",
    "                 test[['pickup_latitude','pickup_longitude']].values,\n",
    "                 test[['dropoff_latitude','dropoff_longitude']].values))\n",
    "\n",
    "pca=PCA().fit(coords)#主成份分析\n",
    "train['pickup_pca0']=pca.transform(train[['pickup_latitude','pickup_longitude']])[:,0]#特征1的成分\n",
    "train['pickup_pca1']=pca.transform(train[['pickup_latitude','pickup_longitude']])[:,1]#特征2的成分\n",
    "train['dropoff_pca0']=pca.transform(train[['dropoff_latitude','dropoff_longitude']])[:,0]\n",
    "train['dropoff_pca1']=pca.transform(train[['dropoff_latitude','dropoff_longitude']])[:,1]\n",
    "test['pickup_pca0']=pca.transform(test[['pickup_latitude','pickup_longitude']])[:,0]#特征1的成分\n",
    "test['pickup_pca1']=pca.transform(test[['pickup_latitude','pickup_longitude']])[:,1]#特征2的成分\n",
    "test['dropoff_pca0']=pca.transform(test[['dropoff_latitude','dropoff_longitude']])[:,0]\n",
    "test['dropoff_pca1']=pca.transform(test[['dropoff_latitude','dropoff_longitude']])[:,1]\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#对store_and_fwd_flag特征进行处理，对N的取0，否则取1\n",
    "#-----------------------------------------\n",
    "train['store_and_fwd_flag_int']=np.where(train['store_and_fwd_flag']=='N',0,1)\n",
    "test['store_and_fwd_flag_int']=np.where(test['store_and_fwd_flag']=='N',0,1)\n",
    "#-----------------------------------------\n",
    "\n",
    "#选取参与训练的特征\n",
    "#-----------------------------------------\n",
    "feature_names=list(train.columns)\n",
    "print('different features in train and test are {}'.format(np.setdiff1d(train.columns,test.columns)))\n",
    "\n",
    "do_not_use_for_training=['pickup_datetime','id','dropoff_datetime','store_and_fwd_flag','trip_duration']\n",
    "feature_names=[f for f in train.columns if f not in do_not_use_for_training]\n",
    "print('we will be using following features for training {}'.format(feature_names))\n",
    "print('')\n",
    "print('total number of features are {}'.format(len(feature_names)))\n",
    "\n",
    "y=np.log(train['trip_duration'].values+1)\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#通过train_test_split对需要训练的数据进行划分，选择测试数据20%，将训练数据、验证数据、测试数据存放为xgb.DMatrix形式，\n",
    "#利用xgb建立训练模型，然后评估模型的RMSLE分数（均方根对数误差）\n",
    "#-----------------------------------------\n",
    "#对train进行训练数据和测试数据分割\n",
    "xtr,xv,ytr,yv=train_test_split(train[feature_names].values,y,test_size=0.2,random_state=1987)\n",
    "dtrain=xgb.DMatrix(xtr,label=ytr)\n",
    "dvalid=xgb.DMatrix(xv,label=yv)\n",
    "dtest=xgb.DMatrix(test[feature_names].values)\n",
    "watch=[(dtrain,'train'),(dvalid,'valide')]\n",
    "xgb_pars={'min_child_weight':50,'eta':0.3,'colsample_bytree':0.3,'max_depth':10,'subsample':0.8,\n",
    "          'lambda':1.,'nthread':-1,'booster':'gbtree','silent':1,'eval_metric':'rmse','objective':'reg:linear'}\n",
    "model=xgb.train(xgb_pars,dtrain,15,watch,early_stopping_rounds=2,maximize=False,verbose_eval=1)\n",
    "print('modeling RMSLE %.5f'% model.best_score)\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#从weather数据中添加天气的影响特征\n",
    "#-----------------------------------------\n",
    "#考虑天气对trip_duration的影响\n",
    "weather=pd.read_csv(r'C:\\Users\\Wizza\\Documents\\Python Scripts\\trip duration\\weather_data_nyc_centralpark_2016.csv')\n",
    "\n",
    "\n",
    "#可视化，日期与最高温度，最低温度的变化曲线\n",
    "#-----------------------------------------\n",
    "from ggplot import *\n",
    "weather['date']=pd.to_datetime(weather.date)\n",
    "weather['day_of_year']=weather['date'].dt.dayofyear\n",
    "p=ggplot(aes(x='date'),data=weather)+geom_line(aes(y='minimum temperature',colour='blue'))+geom_line(aes(y='maximum temperature',colour='red'))\n",
    "p+geom_point(aes(y='minimum temperature',colour='blue'))\n",
    "#findings\n",
    "#二月份的最小温度达到了零下，发现trip_duration比其他时间多\n",
    "\n",
    "#可视化，日期与trip平均时间的变化曲线，以此分析温度与trip时间的关系\n",
    "#-----------------------------------------\n",
    "train_plot=train[['pickup_datetime','trip_duration']]\n",
    "train_plot['pickup_date']=train_plot['pickup_datetime'].dt.date\n",
    "train_plot.drop('pickup_datetime',axis=1,inplace=True)\n",
    "train_plot['trip_duration']=np.log(train_plot['trip_duration'])\n",
    "train_plot_grouped=train_plot.groupby('pickup_date')['trip_duration'].mean()\n",
    "train_plot_grouped=pd.DataFrame(train_plot_grouped)\n",
    "train_plot_grouped.reset_index(inplace=True)\n",
    "train_plot_grouped.rename(columns={'trip_duration':'trip_duration_log'},inplace=True)\n",
    "train_plot_grouped['pickup_date']=pd.to_datetime(train_plot_grouped.pickup_date)\n",
    "p1=ggplot(aes(x='pickup_date'),data=train_plot_grouped)+geom_line(aes(y='trip_duration_log',colour='blue'))\n",
    "p1+geom_point(aes(y='trip_duration_log',colour='blue'))\n",
    "#-----------------------------------------\n",
    "\n",
    "#添加降水量，降雪量和降雪深度的特征\n",
    "#-----------------------------------------\n",
    "weather['precipitation'].unique()\n",
    "weather['precipitation']=np.where(weather['precipitation']=='T',0,weather['precipitation'])\n",
    "weather['precipitation']=list(map(float,weather['precipitation']))\n",
    "weather['snow fall']=np.where(weather['snow fall']=='T',0,weather['snow fall'])\n",
    "weather['snow fall']=list(map(float,weather['snow fall']))\n",
    "weather['snow depth']=np.where(weather['snow depth']=='T',0,weather['snow depth'])\n",
    "weather['snow depth']=list(map(float,weather['snow depth']))\n",
    "\n",
    "\n",
    "#可视化，降水量、降雪量与降雪深度随时间的Scatter图\n",
    "#-----------------------------------------\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "\n",
    "random_x=weather['date'].values\n",
    "random_y0=weather['precipitation']\n",
    "random_y1=weather['snow fall']\n",
    "random_y2=weather['snow depth']\n",
    "\n",
    "trace0=go.Scatter(x=random_x,y=random_y0,mode='markers',name='precipitation')\n",
    "trace1=go.Scatter(x=random_x,y=random_y1,mode='markers',name='snow fall')\n",
    "trace2=go.Scatter(x=random_x,y=random_y2,mode='markers',name='snow depth')\n",
    "data=[trace0,trace1,trace2]\n",
    "\n",
    "plotly.offline.iplot(data, filename='scatter-mode')\n",
    "p1=ggplot(aes(x='pickup_date'),data=train_plot_grouped)+geom_line(aes(y='trip_duration_log',colour='blue'))\n",
    "#-----------------------------------------\n",
    "\n",
    "#增加转角方向的特征，straigth、left、right的个数特征\n",
    "#-----------------------------------------\n",
    "def freq_turn(step_dir):\n",
    "    #功能是获得step_dir的每一个方向的个数\n",
    "    from collections import Counter\n",
    "    step_dir_new=step_dir.split('|')\n",
    "    a_list=Counter(step_dir_new).most_common()\n",
    "    path={}\n",
    "    for i in range(len(a_list)):\n",
    "        path.update({a_list[i]})\n",
    "    a=0\n",
    "    b=0\n",
    "    c=0\n",
    "    if 'straigth' in (path.keys()):\n",
    "        a=path['straigth']\n",
    "    if 'left' in (path.keys()):\n",
    "        b=path['left']\n",
    "    if 'right' in (path.keys()):\n",
    "        c=path['right']\n",
    "    return a,b,c\n",
    "\n",
    "train_fr['straigth']=0\n",
    "train_fr['left']=0\n",
    "train_fr['right']=0\n",
    "\n",
    "train_fr['straigth'],train_fr['left'],train_fr['right']=zip(*train_fr['step_direction'].map(freq_turn))\n",
    "train_fr_new=train_fr[['id','straigth','left','right']]\n",
    "train=pd.merge(train,train_fr_new,on='id',how='left')\n",
    "#-----------------------------------------\n",
    "\n",
    "#增加上车地点到下车中心hvsine 、下车地点到上车中心hvsine值的两个特征\n",
    "#-----------------------------------------\n",
    "train['pickup_datetime']=pd.to_datetime(train['pickup_datetime'])\n",
    "train['date']=train['pickup_datetime'].dt.date\n",
    "train=pd.merge(train,weather[['date','minimum temperature', 'precipitation', 'snow fall', 'snow depth']],\n",
    "               on='date',how='left')\n",
    "train.loc[:,'hvsine_pick_cent_d'] = haversine_(train['pickup_latitude'].values, \n",
    "         train['pickup_longitude'].values, train['centroid_drop_lat'].values, train['centroid_drop_long'].values)\n",
    "train.loc[:,'hvsine_drop_cent_p'] = haversine_(train['dropoff_latitude'].values,\n",
    "         train['dropoff_longitude'].values, train['centroid_pick_lat'].values, train['centroid_pick_long'].values)\n",
    "\n",
    "test.loc[:,'hvsine_pick_cent_d'] = haversine_(test['pickup_latitude'].values, test['pickup_longitude'].values, \n",
    "        test['centroid_drop_lat'].values, test['centroid_drop_long'].values)\n",
    "test.loc[:,'hvsine_drop_cent_p'] = haversine_(test['dropoff_latitude'].values, test['dropoff_longitude'].values, \n",
    "        test['centroid_pick_lat'].values, test['centroid_pick_long'].values)\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#可视化，画出total_distance在0~95距离之间每一段的个数分布，同理distance_pick_cp_drop、distance_pick_cd_drop、distance_pick_cp_cd_drop\n",
    "#-----------------------------------------\n",
    "#分析新特性的影响\n",
    "temp=train[['hvsine_drop_cent_p','hvsine_pick_cent_d','hvsine_drop_cent_d','hvsine_pick_cent_p','hvsine_pick_drop',\n",
    "            'hvsine_cent_p_cent_d','total_distance']]\n",
    "temp.total_distance.dropna(inplace=True)\n",
    "print('total_number of nulls:'.format(temp.total_distance.isnull().sum()))\n",
    "temp['distance_pick_cp_cd_drop']=temp['hvsine_pick_cent_p']+temp['hvsine_cent_p_cent_d']+temp['hvsine_drop_cent_d']\n",
    "temp['distance_pick_cd_drop']=temp['hvsine_pick_cent_d']+temp['hvsine_drop_cent_d']\n",
    "temp['distance_pick_cp_drop']=temp['hvsine_pick_cent_p']+temp['hvsine_drop_cent_p']\n",
    "temp['total_distance']=np.floor(temp['total_distance']/1000)\n",
    "temp['distance_pick_cp_drop']=np.floor(temp['distance_pick_cp_drop'])\n",
    "temp['distance_pick_cd_drop']=np.floor(temp['distance_pick_cd_drop'])\n",
    "temp['distance_pick_cp_cd_drop']=np.floor(temp['distance_pick_cp_cd_drop'])\n",
    "temp1=temp.copy()\n",
    "temp=temp1.sample(100000)\n",
    "aggregation={'distance_pick_cp_cd_drop':'count','distance_pick_cp_drop':'count','distance_pick_cd_drop':'count'}\n",
    "temp2=pd.DataFrame(temp.groupby('total_distance').agg(aggregation))\n",
    "x_plot=np.linspace(0,temp.total_distance.max(),temp.shape[0])\n",
    "temp2.rename(columns={'total_distance':'count'},inplace=True)\n",
    "temp2.reset_index(inplace=True)\n",
    "temp2.total_distance=list(map(int,temp2.total_distance))#map后生成map对象，若显示结果，需要在生成map时使用List\n",
    "x_plot=temp.total_distance.unique()\n",
    "a=np.histogram(temp[['total_distance']].values,list(range(0,95)))\n",
    "N=temp.shape[0]\n",
    "data=[]\n",
    "trace1=go.Scatter(x=a[1],y=a[0],mode='lines',fill='tozeroy',line={'color': 'black', 'width': 2},name='Total_distance_OSRM')\n",
    "data.append(trace1)\n",
    "\n",
    "for kernel in ['distance_pick_cp_cd_drop','distance_pick_cd_drop','distance_pick_cp_drop']:\n",
    "    trace2=go.Scatter(x=a[1],y=np.histogram(temp[[kernel]].values,range(0,95))[0],mode='lines',line=dict(width=2,dash='dash'),name=kernel)\n",
    "    data.append(trace2)\n",
    "layout=go.Layout(annotations=[dict(x=6,y=0.38,showarrow=False,text='N={0} points'.format(N))],\n",
    "                              xaxis=dict(zeroline=False),hovermode='closest')\n",
    "fig=go.Figure(data=data,layout=layout)\n",
    "plotly.offline.iplot(fig)\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "#画出相关系数热图（heatmap）\n",
    "#-----------------------------------------\n",
    "sns.set(style='white')\n",
    "temp3=train.copy()\n",
    "corr=temp3.corr()\n",
    "mask=np.zeros_like(corr,dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "f,ax=plt.subplots(figsize=(15,13))\n",
    "cmap=sns.diverging_palette(220,10,as_cmap=True)\n",
    "sns.heatmap(corr,mask=mask,cmap=cmap,vmax=3,center=0,square=True,linewidths=5,cbar_kws=dict(shrink=5))\n",
    "#-----------------------------------------\n",
    "\n",
    "\n",
    "test_fr=fastest_routes_test.copy()\n",
    "test_fr['straight']=0\n",
    "test_fr['left']=0\n",
    "test_fr['right']=0\n",
    "test_fr['straight'],test_fr['left'],test_fr['right']=zip(*test_fr['step_direction'].map(freq_turn))\n",
    "test_fr_new=test_fr[['id','straight','left','right']]\n",
    "test=pd.merge(test,test_fr_new,on='id',how='left')\n",
    "print(test.columns.shape[0])\n",
    "\n",
    "test['pickup_datetime']=pd.to_datetime(test['pickup_datetime'])\n",
    "test['date']=test['pickup_datetime'].dt.date\n",
    "test['date']=pd.to_datetime(test['date'])\n",
    "test=pd.merge(test,weather[['date','minimum temperature','precipitation','snow fall','snow depth']],\n",
    "              on='date',how='left')\n",
    "\n",
    "\n",
    "#用模型进行验证和预测\n",
    "#-----------------------------------------\n",
    "yvalid=model.predict(dvalid)\n",
    "ytest=model.predict(dtest)\n",
    "\n",
    "fig,ax=plt.subplots(nrows=2,sharex=True,sharey=True)\n",
    "sns.distplot(yvalid,ax=ax[0],color='blue',label='Validation')\n",
    "sns.distplot(ytest,ax=ax[1],color='green',label='Test')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
